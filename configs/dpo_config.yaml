# DPO Training Configuration
# Optimized for Google Colab T4 GPU (15GB VRAM)

# =============================================================================
# Model Configuration
# =============================================================================
model:
  base_model: "meta-llama/Llama-3.1-8B-Instruct"
  # Requires HuggingFace token with access to Llama-3.1

# =============================================================================
# 4-bit Quantization (QLoRA)
# =============================================================================
quantization:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"              # NormalFloat4 quantization
  bnb_4bit_compute_dtype: "bfloat16"      # Compute dtype for 4-bit base models
  bnb_4bit_use_double_quant: true         # Nested quantization for memory savings

# =============================================================================
# LoRA Configuration
# =============================================================================
lora:
  r: 16                                    # LoRA rank (low rank approximation)
  lora_alpha: 32                           # LoRA alpha (scaling factor)
  lora_dropout: 0.05                       # Dropout for LoRA layers
  bias: "none"                             # Bias type for LoRA
  task_type: "CAUSAL_LM"                   # Task type
  target_modules:                          # Modules to apply LoRA
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    # Uncomment for broader coverage (uses more memory):
    # - "gate_proj"
    # - "up_proj"
    # - "down_proj"

# =============================================================================
# DPO Training Parameters
# =============================================================================
training:
  # Epochs
  num_train_epochs: 3

  # Batch size configuration (effective batch = per_device * gradient_accumulation)
  per_device_train_batch_size: 2           # T4 optimized: small batch
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8           # Effective batch size = 2 * 8 = 16

  # Learning rate
  learning_rate: 5.0e-5
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1

  # DPO-specific parameters
  beta: 0.1                                # DPO temperature (lower = stronger preference)
  loss_type: "sigmoid"                     # DPO loss type: sigmoid, hinge, ipo

  # Sequence length limits (memory optimization)
  max_length: 1024                         # Maximum combined length
  max_prompt_length: 512                   # Maximum prompt length

  # Memory optimization
  gradient_checkpointing: true             # Trade compute for memory
  fp16: true                               # Mixed precision training
  # bf16: false                            # Use fp16 on T4 (no bf16 support)

  # Optimizer
  optim: "paged_adamw_32bit"               # Memory-efficient optimizer

  # Logging and saving
  logging_steps: 10
  save_strategy: "epoch"
  eval_strategy: "epoch"
  save_total_limit: 2                      # Keep only last 2 checkpoints

  # Misc
  remove_unused_columns: false
  dataloader_num_workers: 2
  seed: 42

# =============================================================================
# Data Configuration
# =============================================================================
data:
  train_file: "data/dpo/train.jsonl"
  eval_file: "data/dpo/eval.jsonl"
  prompt_column: "prompt"
  chosen_column: "chosen"
  rejected_column: "rejected"

# =============================================================================
# Output Configuration
# =============================================================================
output:
  output_dir: "outputs/dpo-training"
  adapter_dir: "outputs/dpo-adapter"       # LoRA adapter weights
  merged_dir: "outputs/merged-model"       # Merged model (after training)
  gguf_output: "models/psychologist-8b-q4_k_m.gguf"

# =============================================================================
# GGUF Conversion Configuration
# =============================================================================
gguf:
  quantization_type: "Q4_K_M"              # Quantization method
  # Available options:
  # - Q4_0: Fast, lowest quality
  # - Q4_K_M: Good balance of speed and quality (recommended)
  # - Q5_K_M: Higher quality, slower
  # - Q8_0: Highest quality, largest size

# =============================================================================
# Wandb Configuration (Optional)
# =============================================================================
wandb:
  enabled: false                           # Set to true to enable logging
  project: "psychologist-dpo"
  entity: null                             # Your wandb username/team
  run_name: null                           # Auto-generated if not set

# =============================================================================
# Resource Estimates (T4 GPU)
# =============================================================================
# Expected resource usage:
# - GPU Memory: ~13-14 GB (with gradient checkpointing)
# - Training time: ~2-3 hours for 3 epochs on 2000 samples
# - Disk space: ~10 GB for checkpoints

# =============================================================================
# Troubleshooting Memory Issues
# =============================================================================
# If you encounter OOM errors, try these adjustments in order:
# 1. Reduce per_device_train_batch_size to 1
# 2. Increase gradient_accumulation_steps to 16
# 3. Reduce max_length to 768
# 4. Reduce lora r to 8 and lora_alpha to 16
# 5. Remove some target_modules (keep only q_proj, v_proj)
