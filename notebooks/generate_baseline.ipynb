{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Response Generation\n",
    "\n",
    "This notebook generates baseline responses using Llama-3.1-8B-Instruct for DPO training.\n",
    "\n",
    "**Environment**: Google Colab with T4 GPU (15GB VRAM)\n",
    "\n",
    "## Workflow\n",
    "1. Install dependencies\n",
    "2. Load cleaned dataset (from GitHub or upload)\n",
    "3. Load Llama-3.1-8B-Instruct with 4-bit quantization\n",
    "4. Generate baseline responses (simple prompt, no professional guidance)\n",
    "5. Save results with checkpoint support\n",
    "6. Download results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Fix torch/torchvision version conflict and accelerate compatibility\n",
    "!pip uninstall -y torchvision torchaudio -q\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q transformers==4.44.0 accelerate==0.33.0 bitsandbytes==0.43.0 datasets\n",
    "\n",
    "# IMPORTANT: After running this cell, RESTART the runtime!\n",
    "# Go to Runtime -> Restart runtime, then skip this cell and continue from the next one\n",
    "print(\"Installation complete! Please restart runtime now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nCONFIG = {\n    # Model settings\n    \"model_name\": \"meta-llama/Llama-3.1-8B-Instruct\",\n    \n    # Data paths\n    \"input_file\": \"data/processed/counsel_chat_cleaned.jsonl\",\n    \"output_file\": \"data/baseline/responses.jsonl\",\n    \"checkpoint_file\": \"data/baseline/checkpoint.json\",\n    \n    # Generation settings (OPTIMIZED for speed)\n    \"batch_size\": 2,              # Smaller batch for stability\n    \"max_new_tokens\": 256,        # Reduced from 512 - still sufficient for responses\n    \"temperature\": 0.7,\n    \"top_p\": 0.9,\n    \"do_sample\": True,\n    \n    # Checkpoint frequency (save every N records)\n    \"checkpoint_freq\": 50\n}\n\n# Create directories\nos.makedirs(\"data/baseline\", exist_ok=True)\nos.makedirs(\"data/processed\", exist_ok=True)\n\nprint(f\"Config loaded. Estimated time: ~3-4 hours for full dataset\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset\n",
    "\n",
    "**Option A**: Clone from GitHub  \n",
    "**Option B**: Upload file directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Clone from GitHub\n",
    "!git clone https://github.com/yuchangyuan1/6895_project_Agent.git temp_repo\n",
    "!cp temp_repo/data/processed/counsel_chat_cleaned.jsonl data/processed/\n",
    "!rm -rf temp_repo\n",
    "\n",
    "# Option B: Upload file directly\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Upload counsel_chat_cleaned.jsonl\n",
    "# !mv counsel_chat_cleaned.jsonl data/processed/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath: str) -> list:\n",
    "    \"\"\"Load dataset from JSONL file.\"\"\"\n",
    "    records = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            records.append(json.loads(line.strip()))\n",
    "    print(f\"Loaded {len(records)} records from {filepath}\")\n",
    "    return records\n",
    "\n",
    "# Load data\n",
    "dataset = load_dataset(CONFIG[\"input_file\"])\n",
    "print(f\"Sample record: {dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model with 4-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace login (required for Llama models)\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Enter your HuggingFace token\n",
    "# Get token from: https://huggingface.co/settings/tokens\n",
    "HF_TOKEN = \"\"  \n",
    "\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"Logged in to HuggingFace\")\n",
    "else:\n",
    "    print(\"WARNING: No HuggingFace token provided. You may need to login manually.\")\n",
    "    # login()  # Interactive login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name: str):\n",
    "    \"\"\"Load model with 4-bit quantization for memory efficiency.\"\"\"\n",
    "    \n",
    "    # 4-bit quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Loading tokenizer: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    print(f\"Loading model with 4-bit quantization: {model_name}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Print memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        memory_used = torch.cuda.memory_allocated() / 1024**3\n",
    "        print(f\"GPU memory used: {memory_used:.2f} GB\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Load model\n",
    "model, tokenizer = load_model_and_tokenizer(CONFIG[\"model_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline Prompt Template\n",
    "\n",
    "**Important**: This is a simple prompt WITHOUT professional mental health guidance.  \n",
    "This simulates a basic model response that will serve as the \"rejected\" response in DPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_prompt(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a simple baseline prompt without professional guidance.\n",
    "    \n",
    "    This prompt intentionally lacks:\n",
    "    - Empathy instructions\n",
    "    - Professional counseling guidelines\n",
    "    - Safety considerations\n",
    "    \"\"\"\n",
    "    return f\"\"\"You are a helpful assistant. Please respond to the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "\n",
    "def format_for_llama(prompt: str) -> str:\n",
    "    \"\"\"Format prompt for Llama-3.1 chat template.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "# Test prompt\n",
    "sample_prompt = create_baseline_prompt(dataset[0][\"question\"])\n",
    "formatted = format_for_llama(sample_prompt)\n",
    "print(\"Sample formatted prompt:\")\n",
    "print(formatted[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompts: list, model, tokenizer, config: dict) -> list:\n",
    "    \"\"\"Generate responses for a batch of prompts.\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=config[\"max_new_tokens\"],\n",
    "            temperature=config[\"temperature\"],\n",
    "            top_p=config[\"top_p\"],\n",
    "            do_sample=config[\"do_sample\"],\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode responses (only new tokens)\n",
    "    responses = []\n",
    "    for i, output in enumerate(outputs):\n",
    "        # Get only the generated part\n",
    "        input_len = inputs[\"input_ids\"][i].shape[0]\n",
    "        response = tokenizer.decode(\n",
    "            output[input_len:],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "        responses.append(response)\n",
    "    \n",
    "    return responses\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file: str) -> int:\n",
    "    \"\"\"Load checkpoint to resume from last position.\"\"\"\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            return data.get(\"last_index\", 0)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def save_checkpoint(checkpoint_file: str, last_index: int):\n",
    "    \"\"\"Save checkpoint.\"\"\"\n",
    "    with open(checkpoint_file, \"w\") as f:\n",
    "        json.dump({\"last_index\": last_index, \"timestamp\": str(datetime.now())}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Baseline Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_baseline_generation(dataset: list, model, tokenizer, config: dict):\n    \"\"\"\n    Run baseline generation with checkpoint support.\n    \n    Saves results incrementally to support interruption and resume.\n    \"\"\"\n    output_file = config[\"output_file\"]\n    checkpoint_file = config[\"checkpoint_file\"]\n    batch_size = config[\"batch_size\"]\n    checkpoint_freq = config[\"checkpoint_freq\"]\n    \n    # Sort dataset by question length to minimize padding waste\n    print(\"Sorting dataset by question length for efficient batching...\")\n    sorted_dataset = sorted(dataset, key=lambda x: len(x[\"question\"]))\n    print(f\"Length range: {len(sorted_dataset[0]['question'])} - {len(sorted_dataset[-1]['question'])} chars\")\n    \n    # Load checkpoint\n    start_index = load_checkpoint(checkpoint_file)\n    if start_index > 0:\n        print(f\"Resuming from index {start_index}\")\n    \n    # Open output file in append mode if resuming\n    mode = \"a\" if start_index > 0 else \"w\"\n    \n    total = len(sorted_dataset)\n    \n    with open(output_file, mode, encoding=\"utf-8\") as f:\n        for i in tqdm(range(start_index, total, batch_size), desc=\"Generating\"):\n            # Get batch\n            batch = sorted_dataset[i:min(i + batch_size, total)]\n            \n            # Prepare prompts\n            prompts = [\n                format_for_llama(create_baseline_prompt(record[\"question\"]))\n                for record in batch\n            ]\n            \n            # Generate responses\n            try:\n                responses = generate_response(prompts, model, tokenizer, config)\n                \n                # Save results\n                for j, (record, response) in enumerate(zip(batch, responses)):\n                    result = {\n                        \"id\": record[\"id\"],\n                        \"question\": record[\"question\"],\n                        \"original_answer\": record[\"answer\"],\n                        \"baseline_response\": response,\n                        \"topic\": record.get(\"topic\", \"general\")\n                    }\n                    f.write(json.dumps(result, ensure_ascii=False) + \"\\n\")\n                \n                # Flush to disk\n                f.flush()\n                \n            except Exception as e:\n                print(f\"Error at batch starting {i}: {e}\")\n                save_checkpoint(checkpoint_file, i)\n                raise\n            \n            # Save checkpoint periodically\n            if (i + batch_size) % checkpoint_freq == 0:\n                save_checkpoint(checkpoint_file, i + batch_size)\n            \n            # Clear GPU cache periodically\n            if (i + batch_size) % (batch_size * 10) == 0:\n                torch.cuda.empty_cache()\n                gc.collect()\n    \n    # Final checkpoint\n    save_checkpoint(checkpoint_file, total)\n    print(f\"\\nGeneration complete! Output saved to {output_file}\")\n\n# Run generation\nrun_baseline_generation(dataset, model, tokenizer, CONFIG)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Verify Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output file\n",
    "!wc -l {CONFIG[\"output_file\"]}\n",
    "!head -3 {CONFIG[\"output_file\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect results\n",
    "def inspect_results(filepath: str, n: int = 3):\n",
    "    \"\"\"Inspect generated results.\"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= n:\n",
    "                break\n",
    "            record = json.loads(line)\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"ID: {record['id']}\")\n",
    "            print(f\"\\nQuestion: {record['question'][:200]}...\")\n",
    "            print(f\"\\nOriginal Answer: {record['original_answer'][:200]}...\")\n",
    "            print(f\"\\nBaseline Response: {record['baseline_response'][:200]}...\")\n",
    "\n",
    "inspect_results(CONFIG[\"output_file\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Download directly\n",
    "from google.colab import files\n",
    "files.download(CONFIG[\"output_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Save to Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !cp {CONFIG[\"output_file\"]} /content/drive/MyDrive/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory\n",
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}