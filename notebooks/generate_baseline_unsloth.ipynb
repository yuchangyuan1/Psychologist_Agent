{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Response Generation (Unsloth Version)\n",
    "\n",
    "Uses **Unsloth** for 2-5x faster inference.\n",
    "\n",
    "**Environment**: Google Colab T4 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "CONFIG = {\n    \"model_name\": \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n    \"input_file\": \"data/processed/counsel_chat_augmented.jsonl\",\n    \"output_file\": \"data/baseline/responses_augmented.jsonl\",\n    \"checkpoint_file\": \"data/baseline/checkpoint.json\",\n    \"batch_size\": 8,\n    \"max_new_tokens\": 256,\n    \"temperature\": 0.7,\n    \"top_p\": 0.9,\n    \"do_sample\": True,\n    \"checkpoint_freq\": 100\n}\n\nos.makedirs(\"data/baseline\", exist_ok=True)\nos.makedirs(\"data/processed\", exist_ok=True)\nprint(f\"Config loaded. batch_size={CONFIG['batch_size']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!git clone https://github.com/yuchangyuan1/6895_project_Agent.git temp_repo\n!cp temp_repo/data/processed/counsel_chat_augmented.jsonl data/processed/\n!rm -rf temp_repo\nprint(\"Augmented dataset loaded!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath: str) -> list:\n",
    "    records = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            records.append(json.loads(line.strip()))\n",
    "    print(f\"Loaded {len(records)} records\")\n",
    "    return records\n",
    "\n",
    "dataset = load_dataset(CONFIG[\"input_file\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=CONFIG[\"model_name\"],\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "print(f\"Model loaded! GPU: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_prompt(question: str) -> str:\n",
    "    return f\"\"\"You are a helpful assistant. Please respond to the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "\n",
    "def format_for_llama(prompt: str, tokenizer) -> str:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "\n",
    "def generate_batch(prompts: list, model, tokenizer, config: dict) -> list:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=config[\"max_new_tokens\"],\n",
    "            temperature=config[\"temperature\"],\n",
    "            top_p=config[\"top_p\"],\n",
    "            do_sample=config[\"do_sample\"],\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    responses = []\n",
    "    for i, output in enumerate(outputs):\n",
    "        input_len = inputs[\"input_ids\"][i].shape[0]\n",
    "        response = tokenizer.decode(output[input_len:], skip_special_tokens=True).strip()\n",
    "        responses.append(response)\n",
    "\n",
    "    return responses\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file: str) -> int:\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, \"r\") as f:\n",
    "            return json.load(f).get(\"last_index\", 0)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def save_checkpoint(checkpoint_file: str, last_index: int):\n",
    "    with open(checkpoint_file, \"w\") as f:\n",
    "        json.dump({\"last_index\": last_index, \"timestamp\": str(datetime.now())}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Delete old files to start fresh\n",
    "# !rm -f data/baseline/checkpoint.json data/baseline/responses.jsonl\n",
    "# print(\"Old files deleted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline_generation(dataset: list, model, tokenizer, config: dict):\n",
    "    output_file = config[\"output_file\"]\n",
    "    checkpoint_file = config[\"checkpoint_file\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    checkpoint_freq = config[\"checkpoint_freq\"]\n",
    "\n",
    "    start_index = load_checkpoint(checkpoint_file)\n",
    "    if start_index > 0:\n",
    "        print(f\"Resuming from index {start_index}\")\n",
    "\n",
    "    mode = \"a\" if start_index > 0 else \"w\"\n",
    "    total = len(dataset)\n",
    "\n",
    "    with open(output_file, mode, encoding=\"utf-8\") as f:\n",
    "        for i in tqdm(range(start_index, total, batch_size), desc=\"Generating\"):\n",
    "            batch = dataset[i:min(i + batch_size, total)]\n",
    "\n",
    "            prompts = [\n",
    "                format_for_llama(create_baseline_prompt(r[\"question\"]), tokenizer)\n",
    "                for r in batch\n",
    "            ]\n",
    "\n",
    "            try:\n",
    "                responses = generate_batch(prompts, model, tokenizer, config)\n",
    "\n",
    "                for record, response in zip(batch, responses):\n",
    "                    result = {\n",
    "                        \"id\": record[\"id\"],\n",
    "                        \"question\": record[\"question\"],\n",
    "                        \"original_answer\": record[\"answer\"],\n",
    "                        \"baseline_response\": response,\n",
    "                        \"topic\": record.get(\"topic\", \"general\")\n",
    "                    }\n",
    "                    f.write(json.dumps(result, ensure_ascii=False) + \"\\n\")\n",
    "                f.flush()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error at batch {i}: {e}\")\n",
    "                save_checkpoint(checkpoint_file, i)\n",
    "                raise\n",
    "\n",
    "            if (i + batch_size) % checkpoint_freq == 0:\n",
    "                save_checkpoint(checkpoint_file, i + batch_size)\n",
    "\n",
    "    save_checkpoint(checkpoint_file, total)\n",
    "    print(f\"Done! Output: {output_file}\")\n",
    "\n",
    "run_baseline_generation(dataset, model, tokenizer, CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verify Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l data/baseline/responses.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_results(filepath: str, n: int = 2):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= n:\n",
    "                break\n",
    "            record = json.loads(line)\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Q: {record['question'][:150]}...\")\n",
    "            print(f\"Baseline: {record['baseline_response'][:200]}...\")\n",
    "\n",
    "inspect_results(CONFIG[\"output_file\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(CONFIG[\"output_file\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}