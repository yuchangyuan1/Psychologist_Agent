{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Response Generation (Unsloth Version)\n",
    "\n",
    "This notebook uses **Unsloth** for faster inference with Llama-3.1-8B-Instruct.\n",
    "\n",
    "**Environment**: Google Colab with T4 GPU (15GB VRAM)\n",
    "\n",
    "## Advantages over standard transformers\n",
    "- 2-5x faster inference\n",
    "- Lower GPU memory usage\n",
    "- Native 4-bit quantization support\n",
    "\n",
    "## Workflow\n",
    "1. Install Unsloth\n",
    "2. Load cleaned dataset\n",
    "3. Load Llama-3.1-8B-Instruct with Unsloth\n",
    "4. Generate baseline responses\n",
    "5. Download results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install Unsloth (optimized for Colab)\n",
    "!pip install unsloth\n",
    "# Install xformers for additional speedup\n",
    "!pip install xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    \"model_name\": \"unsloth/Meta-Llama-3.1-8B-Instruct\",  # Unsloth optimized version\n",
    "    \n",
    "    # Data paths\n",
    "    \"input_file\": \"data/processed/counsel_chat_cleaned.jsonl\",\n",
    "    \"output_file\": \"data/baseline/responses.jsonl\",\n",
    "    \"checkpoint_file\": \"data/baseline/checkpoint.json\",\n",
    "    \n",
    "    # Generation settings\n",
    "    \"batch_size\": 1,              # Unsloth works best with batch_size=1\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"do_sample\": True,\n",
    "    \n",
    "    # Checkpoint frequency\n",
    "    \"checkpoint_freq\": 100\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"data/baseline\", exist_ok=True)\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "print(\"Config loaded. Unsloth will be 2-5x faster than standard transformers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone from GitHub\n",
    "!git clone https://github.com/yuchangyuan1/6895_project_Agent.git temp_repo\n",
    "!cp temp_repo/data/processed/counsel_chat_cleaned.jsonl data/processed/\n",
    "!rm -rf temp_repo\n",
    "print(\"Dataset loaded from GitHub!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath: str) -> list:\n",
    "    \"\"\"Load dataset from JSONL file.\"\"\"\n",
    "    records = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            records.append(json.loads(line.strip()))\n",
    "    print(f\"Loaded {len(records)} records\")\n",
    "    return records\n",
    "\n",
    "dataset = load_dataset(CONFIG[\"input_file\"])\n",
    "print(f\"Sample: {dataset[0]['question'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model with Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=CONFIG[\"model_name\"],\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,  # Auto-detect\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Enable optimized inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(f\"Model loaded! GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_prompt(question: str) -> str:\n",
    "    \"\"\"Create a simple baseline prompt without professional guidance.\"\"\"\n",
    "    return f\"\"\"You are a helpful assistant. Please respond to the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "\n",
    "def format_for_llama(prompt: str, tokenizer) -> str:\n",
    "    \"\"\"Format prompt for Llama-3.1 chat template.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "# Test\n",
    "sample = format_for_llama(create_baseline_prompt(dataset[0][\"question\"]), tokenizer)\n",
    "print(sample[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single(prompt: str, model, tokenizer, config: dict) -> str:\n",
    "    \"\"\"Generate response for a single prompt using Unsloth.\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=config[\"max_new_tokens\"],\n",
    "            temperature=config[\"temperature\"],\n",
    "            top_p=config[\"top_p\"],\n",
    "            do_sample=config[\"do_sample\"],\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only new tokens\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][input_len:],\n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file: str) -> int:\n",
    "    \"\"\"Load checkpoint to resume.\"\"\"\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, \"r\") as f:\n",
    "            return json.load(f).get(\"last_index\", 0)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def save_checkpoint(checkpoint_file: str, last_index: int):\n",
    "    \"\"\"Save checkpoint.\"\"\"\n",
    "    with open(checkpoint_file, \"w\") as f:\n",
    "        json.dump({\"last_index\": last_index, \"timestamp\": str(datetime.now())}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Delete old checkpoint to start fresh\n",
    "# !rm -f data/baseline/checkpoint.json data/baseline/responses.jsonl\n",
    "# print(\"Old files deleted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline_generation(dataset: list, model, tokenizer, config: dict):\n",
    "    \"\"\"Run baseline generation with Unsloth.\"\"\"\n",
    "    output_file = config[\"output_file\"]\n",
    "    checkpoint_file = config[\"checkpoint_file\"]\n",
    "    checkpoint_freq = config[\"checkpoint_freq\"]\n",
    "    \n",
    "    # Load checkpoint\n",
    "    start_index = load_checkpoint(checkpoint_file)\n",
    "    if start_index > 0:\n",
    "        print(f\"Resuming from index {start_index}\")\n",
    "    \n",
    "    mode = \"a\" if start_index > 0 else \"w\"\n",
    "    total = len(dataset)\n",
    "    \n",
    "    with open(output_file, mode, encoding=\"utf-8\") as f:\n",
    "        for i in tqdm(range(start_index, total), desc=\"Generating\"):\n",
    "            record = dataset[i]\n",
    "            \n",
    "            # Prepare prompt\n",
    "            prompt = format_for_llama(\n",
    "                create_baseline_prompt(record[\"question\"]),\n",
    "                tokenizer\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                # Generate response\n",
    "                response = generate_single(prompt, model, tokenizer, config)\n",
    "                \n",
    "                # Save result\n",
    "                result = {\n",
    "                    \"id\": record[\"id\"],\n",
    "                    \"question\": record[\"question\"],\n",
    "                    \"original_answer\": record[\"answer\"],\n",
    "                    \"baseline_response\": response,\n",
    "                    \"topic\": record.get(\"topic\", \"general\")\n",
    "                }\n",
    "                f.write(json.dumps(result, ensure_ascii=False) + \"\\n\")\n",
    "                f.flush()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error at index {i}: {e}\")\n",
    "                save_checkpoint(checkpoint_file, i)\n",
    "                raise\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (i + 1) % checkpoint_freq == 0:\n",
    "                save_checkpoint(checkpoint_file, i + 1)\n",
    "    \n",
    "    save_checkpoint(checkpoint_file, total)\n",
    "    print(f\"\\nDone! Output: {output_file}\")\n",
    "\n",
    "# Run!\n",
    "run_baseline_generation(dataset, model, tokenizer, CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Verify Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output\n",
    "!wc -l data/baseline/responses.jsonl\n",
    "!head -2 data/baseline/responses.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect results\n",
    "def inspect_results(filepath: str, n: int = 2):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= n:\n",
    "                break\n",
    "            record = json.loads(line)\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Q: {record['question'][:150]}...\")\n",
    "            print(f\"\\nBaseline: {record['baseline_response'][:200]}...\")\n",
    "\n",
    "inspect_results(CONFIG[\"output_file\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(CONFIG[\"output_file\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory\n",
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
